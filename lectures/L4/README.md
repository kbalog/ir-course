# L4: Retrieval evaluation

This lecture deals with the topic of retrieval evaluation. Specifically, we focus on measuring the *effectiveness* of retrieval systems (that is, the "goodness" of rankings), using both offline and online evaluation methodologies. Special attention is given to the introduction of various retrieval measures and the choice of appropriate statistical tests for measuring significance.
Note that the first lecture video focuses on offline evaluation, but the corresponding slide deck has additional information on online evaluation.

## Lecture videos and slides

| **Module** | **Topic** | **Lecture material** | 
| -- | -- | -- | 
| L4-1 | Retrieval evaluation | [lecture video](https://youtu.be/b8KoTQYDjxA), [slides](https://speakerdeck.com/kbalog/information-retrieval-and-text-mining-2021-retrieval-evaluation) |
| L4-2 | A/B testing fundamentals | [external video](https://youtu.be/VpTlNRUcIDo) |
| L4-3 | Statistical significance testing | [external video](https://youtu.be/YEmiWfk9KRo) |

## Reading

  * Text Data Management and Analysis (Zhai & Massung)
    - Chapter 9
  
## Summary

Key concepts in this lecture:

  * Ingredients of offline test collections
  * Collecting relevance assessments (expert judges/crowdsourcing, pooling, binary vs. graded relevance)
  * Set retrieval measures (precision, recall, F1)
  * Ranked retrieval measures (AP, RR, NDCG)
  * Evaluating rankings for multiple queries
  * Online evaluation (A/B testing and interleaving)
  * Statistical significance testing
