{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an inverted index\n",
    "\n",
    "  - You are given a sample (1000 documents) from the [The Reuters-21578 data collection](http://www.daviddlewis.com/resources/testcollections/reuters21578/) in `data/reuters21578-000.xml`\n",
    "  - The code that parses the XML and extract a list of preprocessed terms (tokenized, lowercased, stopwords removed) is already given\n",
    "  - You are also given an `InvertedIndex` class that manages the posting lists operations\n",
    "  - Your task is to build an inverted index from the input collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import re\n",
    "\n",
    "from typing import List, Dict, Union, Any, Callable\n",
    "from collections import Counter, defaultdict\n",
    "from xml.dom import minidom\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping tags inside <> using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striptags(text: str) -> str:\n",
    "    \"\"\"Removes xml tags.\n",
    "\n",
    "    Args:\n",
    "        text: Text string with xml tags.\n",
    "\n",
    "    Returns:\n",
    "        String without xml tags.\n",
    "    \"\"\"\n",
    "    p = re.compile(r\"<.*?>\")\n",
    "    return p.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse input text and return a list of indexable terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text: str) -> List[str]:\n",
    "    \"\"\"Parses documents and removes xml tags and punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: Text to parse.\n",
    "\n",
    "    Returns:\n",
    "        List of tokens.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    # Replace specific characters with space\n",
    "    chars = [\"'\", \".\", \":\", \",\", \"!\", \"?\", \"(\", \")\"]\n",
    "    for ch in chars:\n",
    "        text = text.replace(ch, \" \")\n",
    "\n",
    "    # Remove tags\n",
    "    text = striptags(text)\n",
    "\n",
    "    # Tokenization\n",
    "    # default behavior of the split is to split on one or more whitespaces\n",
    "    return [term.lower() for term in text.split() if term not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input document collection\n",
    "\n",
    "  - The collection is given as a single XML file. \n",
    "  - Each document is inside `<REUTERS ...> </REUTERS>`.\n",
    "  - We extract the contents of the `<DATE>`, `<TITLE>`, and `<BODY>` tags.\n",
    "  - After each extracted document, the provided callback function is called and all document data is passed in a single dict argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_collection(input_file:str, callback: Callable) -> None:\n",
    "    \"\"\"Processes file and calls the callback function for each document in the\n",
    "    file.\n",
    "\n",
    "    Args:\n",
    "        input_file: Path to file to process.\n",
    "        callback: Function that will be called for each document.\n",
    "    \"\"\"\n",
    "    xmldoc = minidom.parse(input_file)\n",
    "    # Iterate documents in the XML file\n",
    "    itemlist = xmldoc.getElementsByTagName(\"REUTERS\")\n",
    "    for doc_id, doc in enumerate(itemlist):\n",
    "        date = doc.getElementsByTagName(\"DATE\")[0].firstChild.nodeValue\n",
    "        # Skip documents without a title or body\n",
    "        if not (doc.getElementsByTagName(\"TITLE\") and doc.getElementsByTagName(\"BODY\")):\n",
    "            continue\n",
    "        title = doc.getElementsByTagName(\"TITLE\")[0].firstChild.nodeValue\n",
    "        body = doc.getElementsByTagName(\"BODY\")[0].firstChild.nodeValue\n",
    "        callback({\n",
    "            \"doc_id\": doc_id+1,\n",
    "            \"date\": date,\n",
    "            \"title\": title,\n",
    "            \"body\": body\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints a document\"s contents (used as a callback function passed to `process_collection`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
    "    \"\"\"Print details of the first 5 documents.\n",
    "\n",
    "    Args:\n",
    "        doc: Dictionary with document details.\n",
    "    \"\"\"\n",
    "    if doc[\"doc_id\"] <= 5:  # print only the first 5 documents\n",
    "        print(\"docID:\", doc[\"doc_id\"])\n",
    "        print(\"date:\", doc[\"date\"])\n",
    "        print(\"title:\", doc[\"title\"])\n",
    "        print(\"body:\", doc[\"body\"])\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_collection(\"data/reuters21578-000.xml\", print_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Complete the inverted index class\n",
    "\n",
    "  - The inverted index is an object with methods for adding and fetching postings.\n",
    "  - The data is stored in a map, where keys are terms and values are lists of postings.\n",
    "  - Each posting is an object that holds the doc_id and an optional payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a simple data class, intializing it can be abstracted with\n",
    "# the use of dataclass decorator.\n",
    "# https://docs.python.org/3/library/dataclasses.html\n",
    "\n",
    "@dataclass\n",
    "class Posting:\n",
    "    doc_id: int\n",
    "    payload: Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index = defaultdict(list)\n",
    "    \n",
    "    def add_posting(self, term: str, doc_id: int, payload: Any=None) -> None:\n",
    "        \"\"\"Adds a document to the posting list of a term.\"\"\"\n",
    "        # append new posting to the posting list\n",
    "        # TODO: append new posting to the posting list\n",
    "\n",
    "    def get_postings(self, term: str) -> List[Posting]:\n",
    "        \"\"\"Fetches the posting list for a given term.\"\"\"\n",
    "        # TODO: complete\n",
    "        return None\n",
    "\n",
    "    def get_terms(self) -> List[str]:\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self._index.keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_postings():\n",
    "    ind = InvertedIndex()\n",
    "    ind.add_posting(\"term\", 1, 1)\n",
    "    ind.add_posting(\"term\", 2, 4)\n",
    "    # Testing existing term\n",
    "    postings = ind.get_postings(\"term\")\n",
    "    assert len(postings) == 2\n",
    "    assert postings[0].doc_id == 1\n",
    "    assert postings[0].payload == 1\n",
    "    assert postings[1].doc_id == 2\n",
    "    assert postings[1].payload == 4\n",
    "    # Testing non-existent term\n",
    "    assert ind.get_postings(\"xyx\") is None\n",
    "\n",
    "def test_vocabulary():\n",
    "    ind = InvertedIndex()\n",
    "    ind.add_posting(\"term1\", 1)\n",
    "    ind.add_posting(\"term2\", 1)\n",
    "    ind.add_posting(\"term3\", 2)\n",
    "    ind.add_posting(\"term2\", 3)\n",
    "    assert set(ind.get_terms()) == set([\"term1\", \"term2\", \"term3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2: Build an inverted index from the input collection\n",
    "\n",
    "**TODO**: Complete the code to index the entire document collection.  (The content for each document should be the title and body concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = InvertedIndex()\n",
    "\n",
    "def index_doc(doc: Dict[str, Union[str, int]]) -> None:\n",
    "    \"\"\"Index document by concatenating document title and body.\n",
    "\n",
    "    Args:\n",
    "        doc: Document details.\n",
    "    \"\"\"\n",
    "    text = doc[\"title\"] + \" \" + doc[\"body\"]\n",
    "    terms = parse(text)  # list of terms in the document\n",
    "    # TODO: index the document (add all terms with freqs using `ind.add_posting()`)\n",
    "    \n",
    "process_collection(\"data/reuters21578-000.xml\", index_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Save the inverted index to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the inverted index to a file (`data/index.dat`). Use a simple text format with `termID docID1:freq1 docID2:freq2 ...` per line, e.g.,\n",
    "\n",
    "```\n",
    "xxx 1:1 2:1 3:2\n",
    "yyy 2:1 4:2\n",
    "zzz 1:3 3:1 5:2\n",
    "...\n",
    "```\n",
    "\n",
    "Implement this by (1) adding a `write_to_file(self, filename)` method to the `InvertedIndex` class and then (2) invoking that method in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (advanced, optional): Plot collection size against index size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot that compares the size of the document collection (bytes) against the size of the corresponding index (bytes) on the y-axis vs. with respect to the number of documents on the x-axis. You may use [Matplotlib](https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm) for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our solution, we create a different callback function and use that one for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6a0b9ba27f634b55723b9a72ccf6e1561be2239a81593bce53747f2fee7a1a2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
