{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment analysis with Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "The Transformer in NLP is an architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies. It is a transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
    "\n",
    "\n",
    "In this exercise you will explore transformers pre-trained on the task for sentiment classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements\n",
    "\n",
    "To install the transformers library run following commands.\n",
    "```\n",
    "conda install -c pytorch pytorch\n",
    "pip install transformers[torch]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Import required packages\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# Create class for data preparation\n",
    "class SimpleDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load tokenizer and model, create trainer\n",
    "model_name = \"siebert/sentiment-roberta-large-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "trainer = Trainer(model=model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create list of texts (can be imported from .csv, .xls etc.)\n",
    "pred_texts = ['I like that','That is annoying','This is great!','WouldnÂ´t recommend it.']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tokenize texts and create prediction data set\n",
    "tokenized_texts = tokenizer(pred_texts,truncation=True,padding=True)\n",
    "pred_dataset = SimpleDataset(tokenized_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run predictions\n",
    "predictions = trainer.predict(pred_dataset)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transform predictions to labels\n",
    "preds = predictions.argmax(-1)\n",
    "labels = pd.Series(preds).map(model.config.id2label)\n",
    "scores = (np.exp(predictions)/np.exp(predictions).sum(-1,keepdims=True)).max(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create DataFrame with texts, predictions, labels, and scores\n",
    "df = pd.DataFrame(list(zip(pred_texts,preds,labels,scores)), columns=['text','pred','label','score'])\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the boundaries\n",
    "\n",
    "Try to find some examples where the model fails."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_prediction(text: str) -> Tuple[str, float]:\n",
    "    \"\"\"Get label and score for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text input to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of label and score for a given text.\n",
    "    \"\"\"\n",
    "    tokenized_texts = tokenizer([text],truncation=True,padding=True)\n",
    "    pred_dataset = SimpleDataset(tokenized_texts)\n",
    "    predictions = trainer.predict(pred_dataset)[0]\n",
    "    label = model.config.id2label[predictions.argmax(-1)[0]]\n",
    "    score = (np.exp(predictions)/np.exp(predictions).sum(-1,keepdims=True)).max(1)[0]\n",
    "    return label, score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label, score = get_prediction(...)\n",
    "print(f\"\\nLabel: {label}\\nScore: {score}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('dat640': conda)"
  },
  "interpreter": {
   "hash": "4f36a626e24aa200704e7a1da8e159d79437a6ae015274136a84a715398481c5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}