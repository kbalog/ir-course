{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece algorithm (top-down)\n",
    "\n",
    "There are two versions of the WordPiece algorithm: Bottom-up and top-down. In both cases goal is the same: \"Given a training corpus and a number of desired tokens D, the optimization problem is to select D wordpieces such that the resulting corpus is minimal in the number of wordpieces when segmented according to the chosen wordpiece model.\"\n",
    "\n",
    "The top-down WordPiece generation algorithm takes in a set of (word, count) pairs and a threshold T, and returns a vocabulary V.\n",
    "\n",
    "The algorithm is iterative. It is run for k iterations, where typically k = 4, but only the first two are really important. The third and fourth (and beyond) are just identical to the second. Note that each step of the binary search runs the algorithm from scratch for k iterations.\n",
    "\n",
    "For this algorithm, we are following the steps as described in the [TensorFlow guide](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "PREFIX = \"##\"\n",
    "UNKNOWN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Every morning we look for shells in the sand. I found fifteen big shells\"\n",
    "\" last year. I put them in a special place in my room. This year I want to learn\"\n",
    "\" to surf. It is hard to surf, but so much fun! My sister is a good surfer. She\"\n",
    "\" says that she can teach me. I hope I can do it!\")\n",
    "\n",
    "text = re.sub(\"[.,!]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter(text.lower().split())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1 steps  \n",
    "\n",
    "  1. Iterate over every word and count pair in the input, denoted as (w, c).\n",
    "  2. For each word w, generate every substring, denoted as s. E.g., for the word human, we generate {h, hu, hum, huma, human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, ##n}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_substrings(word:str, word_split_points:List[int]=None) -> List[str]:\n",
    "    \"\"\"Generate all word substrings. E.g., for the word human, we generate [h, \n",
    "    hu, hum, huma, human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, \n",
    "    ##n]. \n",
    "\n",
    "    If split_points is set, returns only substrings that start at a split point.\n",
    "    E.g., for word human and split_points [0,2], we generate ['h', 'hu', 'hum', \n",
    "    'huma', 'human', '##m', '##ma', '##man']\n",
    "    \n",
    "    Args:\n",
    "        word: Word for which to generate substrings.\n",
    "        word_split_points: List of indices \n",
    "        \n",
    "    Returns:\n",
    "        List of substrings. If a substring dos not include the start of a word, \n",
    "        prepends PREFIX.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_generate_substrings_default():\n",
    "    assert generate_substrings(\"human\") == [\"h\", \"hu\", \"hum\", \"huma\", \"human\", \"##u\", \"##um\", \"##uma\", \"##uman\", \"##m\", \"##ma\", \"##man\", \"##a\", \"##an\", \"##n\"]\n",
    "\n",
    "def test_generate_substrings():\n",
    "    assert generate_substrings(\"human\", [0,2]) == [\"h\", \"hu\", \"hum\", \"huma\", \"human\", \"##m\", \"##ma\", \"##man\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. Maintain a substring-to-count hash map, and increment the count of each s by c. E.g., if we have (human, 113) and (humas, 3) in our input, the count of s = huma will be 113+3=116."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substring_counts(word_tuples:List[Tuple[str, int]], split_points:List[List[int]]=None) -> Dict[str, int]:\n",
    "    \"\"\"Given a list of word-count pairs, returns substrings-count dictionary.\n",
    "    \n",
    "    Args:\n",
    "        word_tuples: List of word-count pairs.\n",
    "        split_points: List of indices at which to generate substrings for each\n",
    "            word.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with substring-count pairs\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"key,count\", [(\"human\", 113), (\"humas\", 3), (\"huma\", 116)])\n",
    "def test_get_substring_counts(key,count):\n",
    "    counts = get_substring_counts([(\"human\", 113), (\"humas\", 3)])\n",
    "    assert counts[key] == count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  4. Once we've collected the counts of every substring, iterate over the (s, c) pairs starting with the longest s first.\n",
    "  5. Keep any s that has a c > T. E.g., if T = 100 and we have (pers, 231); (dogs, 259); (##rint; 76), then we would keep pers and dogs.\n",
    "  6. When an s is kept, subtract off its count from all of its prefixes. This is the reason for sorting all of the s by length in step 4. This is a critical part of the algorithm, because otherwise words would be double counted. For example, let's say that we've kept human and we get to (huma, 116). We know that 113 of those 116 came from human, and 3 came from humas. However, now that human is in our vocabulary, we know we will never segment human into huma ##n. So once human has been kept, then huma only has an effective count of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_for_iteration(substrings_counts: Dict[str, int], threshold: int=3) -> Dict[str, int]:\n",
    "    \"\"\"Given substrings counts, returns a dictionary of elements with count\n",
    "    higher than treshold. Value associated with each key is a unique number \n",
    "    (index)\n",
    "    \n",
    "    Args:\n",
    "        substrings_counts: Dictionary with substring-count pairs.\n",
    "        threshold: Threshold at which to keep substrings.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of kept substrings as keys and incrementing index as value.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.fixture\n",
    "def substrings():\n",
    "    return get_substring_counts(words.items())\n",
    "\n",
    "def test_get_vocabulary_for_iteration_len(substrings):\n",
    "    vocabulary = get_vocabulary_for_iteration(substrings, 3)\n",
    "    assert len(vocabulary) == 33\n",
    "\n",
    "def test_get_vocabulary_for_iteration_3(substrings):\n",
    "    vocabulary = get_vocabulary_for_iteration(substrings, 3)\n",
    "    assert vocabulary[\"she\"] == 1\n",
    "\n",
    "def test_get_vocabulary_for_iteration_2(substrings):\n",
    "    vocabulary = get_vocabulary_for_iteration(substrings, 2)\n",
    "    assert vocabulary[\"year\"] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying WordPiece\n",
    "\n",
    "Once a WordPiece vocabulary has been generated, we need to be able to apply it to new data. The algorithm is a simple greedy longest-match-first application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word:str, vocabulary: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Tokenize single word vocabulary. Returns <unk> token if word cannot be\n",
    "    fully tokinzed due to missing tokens in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        word: Word to tokenize.\n",
    "        vocabulary: Dictionary where keys are tokens and values are unique\n",
    "        numbers associated with them.\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.fixture\n",
    "def vocabulary(substrings):\n",
    "    return get_vocabulary_for_iteration(substrings, 3)\n",
    "\n",
    "@pytest.mark.parametrize(\"word,tokens\", [(\"shells\", [\"she\", \"##l\", \"##l\", \"##s\"]), (\"fishing\", [\"<unk>\"])])\n",
    "def test_tokenize(word, tokens, vocabulary):\n",
    "    assert tokenize(word, vocabulary) == tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2+\n",
    "\n",
    "This algorithm will severely overgenerate word pieces. The reason is that we only subtract off counts of prefix tokens. Therefore, if we keep the word human, we will subtract off the count for h, hu, hu, huma, but not for ##u, ##um, ##uma, ##uman and so on. So we might generate both human and ##uman as word pieces, even though ##uman will never be applied.\n",
    "\n",
    "So why not subtract off the counts for every substring, not just every prefix? Because then we could end up subtracting off the counts multiple times. Let's say that we're processing s of length 5 and we keep both (##denia, 129) and (##eniab, 137), where 65 of those counts came from the word undeniable. If we subtract off from every substring, we would subtract 65 from the substring ##enia twice, even though we should only subtract once. However, if we only subtract off from prefixes, it will correctly only be subtracted once.\n",
    "\n",
    "To solve the overgeneration issue mentioned above, we perform multiple iterations of the algorithm.\n",
    "\n",
    " * Subsequent iterations are identical to the first, with one important distinction: In step 2, instead of considering every substring, we apply the WordPiece tokenization algorithm using the vocabulary from the previous iteration, and only consider substrings which start on a split point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_points(word: str, vocabulary: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Returns list of split points for word. If produced token is <unk>, every\n",
    "    character should be considered a split point.\n",
    "    \n",
    "    Args:\n",
    "        word: Word for which to obtain tokens and infer split points.\n",
    "        vocabulary: Dictionary with available tokens.\n",
    "        \n",
    "    Returns:\n",
    "        List of indices where word was split into tokens. (It should start with\n",
    "        0) \n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"word,splits\", [(\"shells\", [0, 3, 4, 5]), (\"fishing\", [0,1,2,3,4,5,6])])\n",
    "def test_get_split_points(word, splits, vocabulary):\n",
    "    assert get_split_points(word, vocabulary) == splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full implementation\n",
    "For full implementations we do 4 iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(word_tuples: List[Tuple[str, int]], threshold: int) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        word_tuples: Iterable (e.g., list, generator) of word-count pairs.\n",
    "        threshold: Threshold at which to keep substrings.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of kept substrings as keys and incrementing index as value.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_get_vocabulary():\n",
    "    vocabulary = get_vocabulary(words.items(), 2)\n",
    "    assert tokenize(\"year\", vocabulary) == [\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.client import Elasticsearch\n",
    "from collections import Counter\n",
    "\n",
    "es = Elasticsearch(timeout=120)\n",
    "es.indices.put_settings(body={\"max_result_window\": 100000})\n",
    "index = \"trec9_index\"\n",
    "\n",
    "ids = [doc[\"_id\"] for doc in es.search(index=index, query={\"match_all\" : {}}, _source=False, size= 100000)[\"hits\"][\"hits\"]]\n",
    "\n",
    "TREC9_WORD_COUNTS = Counter()\n",
    "for doc_id in ids:\n",
    "    for field in [\"title\", \"body\"]:\n",
    "        terms = es.termvectors(index=index, id=doc_id, fields=field, term_statistics=True).get(\"term_vectors\", {}).get(field, {}).get(\"terms\", {})\n",
    "        TREC9_WORD_COUNTS.update({key: value[\"term_freq\"] for key, value in terms.items()})\n",
    "\n",
    "print(f\"Number of words in corpus: {len(TREC9_WORD_COUNTS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in [5, 100, 1000]:\n",
    "    vocabulary = get_vocabulary(TREC9_WORD_COUNTS.items(), threshold)\n",
    "    print(f\"\\nLength of vocabulary: {len(vocabulary)}\\nTokens:\")\n",
    "    for word in [\"shells\", \"fishing\"]:\n",
    "        print(f\"\\t{word}: {tokenize(word, vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"undeniable\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.fixture\n",
    "def vocabulary():\n",
    "    return get_vocabulary(TREC9_WORD_COUNTS.items(), 100)\n",
    "\n",
    "@pytest.mark.parametrize(\"word,tokens\", [(\"shells\", [\"she\", \"##ll\", \"##s\"]), (\"fishing\", [\"fish\", \"##ing\"])])\n",
    "def test_tokenize(word, tokens, vocabulary):\n",
    "    assert tokenize(word, vocabulary) == tokens"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f36a626e24aa200704e7a1da8e159d79437a6ae015274136a84a715398481c5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
