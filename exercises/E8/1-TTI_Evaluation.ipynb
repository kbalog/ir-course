{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Entity Type Identification Evaluation\n",
    "\n",
    "In this exercise, you'll need to implement lenient evaluation measures for the target entity type identification task.\n",
    "\n",
    "As a reminder, _target entity type identification_ is the task of finding the target types of a given input query, from a type taxonomy, such that these types correspond to most specific types of entities that are relevant to the query.  Target types cannot lie on the same branch in the taxonomy.\n",
    "\n",
    "Our final measure is normalized discounted cumulative gain (NDCG), but we need to compute the gain values of answer types based on their distance from ground truth types in the type taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "from typing import Callable, List, Optional, Set\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type taxonomy\n",
    "\n",
    "We use the DBpedia Ontology as our type taxonomy. It is given to you in a preprocessed format in `data/dbpedia_types.tsv`, where each line corresponds to a type, and the tab-separated columns, respectively, are: type identifier, depth in the hierarchy, and parent type.\n",
    "\n",
    "**TODO** Complete the class below such that you can compute the distance between two types in the taxonomy. You're free to choose how you want to internally represent the type hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeTaxonomy:\n",
    "    \n",
    "    ROOT = \"owl:Thing\"\n",
    "    \n",
    "    def __init__(self, tsv_filename: str) -> None:\n",
    "        \"\"\"Initializes the type taxonomy by loading it from a TSV file.\n",
    "        \n",
    "        Args:\n",
    "            tsv_filename: Name of TSV file, with type_id, depth, and parent_id columns.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def max_depth(self) -> int:\n",
    "        \"\"\"Returns the maximum depth of the type taxonomy.\"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def depth(self, type_id: str) -> int:\n",
    "        \"\"\"Returns the depth of a type in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            The depth of the type in the hierarchy (0 for root).\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def parent(self, type_id: str) -> Optional[str]:\n",
    "        \"\"\"Returns the parent type of a type in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id: Type ID.\n",
    "            \n",
    "        Returns:\n",
    "            Parent type ID, or None if the input type is root.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    def dist(self, type_id1: str, type_id2: str) -> float:\n",
    "        \"\"\"Computes the distance between two types in the taxonomy.\n",
    "        \n",
    "        Args:\n",
    "            type_id1: ID of first type.\n",
    "            type_id2: ID of second type.\n",
    "            \n",
    "        Returns:\n",
    "            The distance between the two types in the type taxonomy, which is\n",
    "            the number of steps between them if they lie on the same branch,\n",
    "            and otherwise `math.inf`.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.fixture\n",
    "def dbpedia_types():\n",
    "    return TypeTaxonomy(\"data/dbpedia_types.tsv\")\n",
    "\n",
    "def test_max_depth(dbpedia_types):\n",
    "    assert dbpedia_types.max_depth() == 7\n",
    "\n",
    "@pytest.mark.parametrize(\"type_id,depth\", [\n",
    "    (\"owl:Thing\", 0),\n",
    "    (\"dbo:Agent\", 1),\n",
    "    (\"dbo:SportFacility\", 4),\n",
    "    (\"dbo:RaceTrack\", 5)\n",
    "])\n",
    "def test_depth(dbpedia_types, type_id, depth):\n",
    "    assert dbpedia_types.depth(type_id) == depth\n",
    "    \n",
    "@pytest.mark.parametrize(\"type_id,parent\", [\n",
    "    (\"owl:Thing\", None),\n",
    "    (\"dbo:Agent\", \"owl:Thing\"),\n",
    "    (\"dbo:SportFacility\", \"dbo:ArchitecturalStructure\"),\n",
    "    (\"dbo:RaceTrack\", \"dbo:SportFacility\")\n",
    "])\n",
    "def test_depth(dbpedia_types, type_id, parent):\n",
    "    assert dbpedia_types.parent(type_id) == parent\n",
    "\n",
    "@pytest.mark.parametrize(\"type_id1,type_id2,distance\", [\n",
    "    (\"dbo:Agent\", \"dbo:Agent\", 0),  # same type\n",
    "    (\"dbo:Agent\", \"dbo:Person\", 1),  # type2 is more specific\n",
    "    (\"dbo:Artist\", \"dbo:Agent\", 2),  # type2 is more generic\n",
    "    (\"dbo:Artist\", \"dbo:Broadcaster\", math.inf)  # different branch\n",
    "])  \n",
    "def test_distance(dbpedia_types, type_id1, type_id2, distance):\n",
    "    assert dbpedia_types.dist(type_id1, type_id2) == distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gain values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, refer to this global variable in the gain computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_taxonomy = TypeTaxonomy(\"data/dbpedia_types.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defined in a _linear_ fashion, the gain of a type is computed as:\n",
    "\n",
    "$$r(y) = \\max_{\\hat{y} \\in \\hat{\\mathcal{T}}_q} \\big( 1 - \\frac{d(y,\\hat{y})}{h} \\big)$$\n",
    "\n",
    "where $\\hat{\\mathcal{T}}_q$ is the set of ground truth types, $\\hat{y}$ is a ground truth type, $y$ is an answer type, $d(y, \\hat{y})$ is the distance between types in the taxonomy, and $h$ is the maximum depth of the type taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_linear(gt_types: Set[str], answer_type_id: str) -> float:\n",
    "    \"\"\"Computes the gain of an answer type in a linear fashion.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth type IDs.\n",
    "        answer_type_id: Answer type ID.\n",
    "    \n",
    "    Returns:\n",
    "        Gain value.\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the gain of an answer type can be defined using an _exponential_ decay function:\n",
    "\n",
    "$$r(y) = \\max_{\\hat{y} \\in \\hat{\\mathcal{T}}_q} \\big ( b^{-d(y,\\hat{y})} \\big )$$\n",
    "\n",
    "where $b$ is the base of the exponential function (here: $b=2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_exponential(gt_types: Set[str], answer_type_id: str) -> float:\n",
    "    \"\"\"Computes the gain of an answer type using exponential decay.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth type IDs.\n",
    "        answer_type_id: Answer type ID.\n",
    "    \n",
    "    Returns:\n",
    "        Gain value.\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: It's your task to write some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Plug the gain values computed using either linear or exponential into the NDCG computation to get a final evaluation score.\n",
    "\n",
    "The DCG and NDCG computation parts are given. The only part that needs completing is the construction of the ideal ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Complete the `get_ideal_ranking` function. Note that unlike in document ranking, the ground truth answers alone are insufficient. They should be complemented by related types along the hierarchy, which would yield partial matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_ranking(gt_types: Set[str]) -> List[str]:\n",
    "    \"\"\"Generates an ideal ranking corresponding to a set of ground truth types.\n",
    "    \n",
    "    Args:\n",
    "        gt_types: Set of ground truth types.\n",
    "    \n",
    "    Returns:\n",
    "        A ranked list of types that constitute an ideal ranking gain-wise.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances: List[float], k: int) -> float:\n",
    "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of types.\n",
    "    \n",
    "    Args:\n",
    "        relevances: List with the relevance levels corresponding to a ranked list of types.\n",
    "        k: Rank cut-off.\n",
    "        \n",
    "    Returns:\n",
    "        DCG@k (float).\n",
    "    \"\"\"\n",
    "    return relevances[0] + sum(\n",
    "        [relevances[i] / math.log(i + 1, 2) \n",
    "         for i in range(1, min(k, len(relevances)))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(system_ranking: List[str], gt_types: Set[str], gain_function: Callable, k:int = 10) -> float:\n",
    "    \"\"\"Computes NDCG@k for a given system ranking.\n",
    "    \n",
    "    Args:\n",
    "        system_ranking: Ranked list of answer type IDs (from most to least relevant).\n",
    "        gt_types: Set of ground truth types.\n",
    "        gain_function: Function for computing the gain of an answer type.\n",
    "        k: Rank cut-off.\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k (float).\n",
    "    \"\"\"\n",
    "    # Relevance (gain) levels for the ranked docs.\n",
    "    relevances = [gain_function(gt_types, type_id) for type_id in system_ranking]\n",
    "\n",
    "    # Relevance levels (gains) of the idealized ranking.\n",
    "    relevances_ideal = [gain_function(gt_types, type_id) \n",
    "                        for type_id in get_ideal_ranking(gt_types)]\n",
    "    \n",
    "    return dcg(relevances, k) / dcg(relevances_ideal, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: It's your task to write some tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
