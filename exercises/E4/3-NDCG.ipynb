{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll have to evaluate system rankings, by computing the Normalized Discounted Cumulative Gain (NDCG) measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import math\n",
    "import pytest\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings produced for each query\n",
    "\n",
    "The key is the query ID (string), the value is a list of document IDs (ints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_rankings = {\n",
    "    \"q1\": [2, 1, 3, 4, 5, 6, 10, 7, 9, 8],\n",
    "    \"q2\": [1, 2, 9, 4, 5, 6, 7, 8, 3, 10],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "\n",
    "The key is the query ID, the value is a dictionary with (document ID, relevance) pairs. Relevance is measured on a 3-point scale: non-relevant (0), poor (1), good (2), excellent (3). Documents not listed here are non-relevant (relevance=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"q1\": {4: 3, 1: 2, 2: 1},\n",
    "    \"q2\": {3: 3, 4: 3, 1: 2, 2: 1, 8: 1},\n",
    "    \"q3\": {1: 3, 4: 3, 7: 2, 5: 2, 6: 1, 8: 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics\n",
    "\n",
    "Discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$DCG_k = rel_1 + \\sum_{i=2}^k\\frac{rel_i}{\\log_2 i}$$\n",
    "\n",
    "Normalized discounted cumulative gain at rank $k$ is computed as:\n",
    "\n",
    "$$NDCG_k = \\frac{DCG_k}{IDCG_k}$$\n",
    "\n",
    "where $IDCG_k$ is the $DCG_k$ score of an idealized (perfect) ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances: Dict[int, int], k: int) -> float:\n",
    "    \"\"\"Computes DCG@k, given the corresponding relevance levels for a ranked list of documents.\n",
    "    \n",
    "    For example, given a ranking [2, 3, 1] where the relevance levels according to the ground \n",
    "    truth are {1:3, 2:4, 3:1}, the input list will be [4, 1, 3].\n",
    "    \n",
    "    Args:\n",
    "        relevances: List with the ground truth relevance levels corresponding to a ranked list of documents.\n",
    "        k: Rank cut-off.\n",
    "        \n",
    "    Returns:\n",
    "        DCG@k (float).\n",
    "    \"\"\"\n",
    "    # TODO: complete\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(system_ranking: List[int], ground_truth: List[int], k:int = 10) -> float:\n",
    "    \"\"\"Computes NDCG@k for a given system ranking.\n",
    "    \n",
    "    Args:\n",
    "        system_ranking: Ranked list of document IDs (from most to least relevant).\n",
    "        ground_truth: Dict with document ID: relevance level pairs. Document not present here are to be taken with relevance = 0.\n",
    "        k: Rank cut-off.\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k (float).\n",
    "    \"\"\"\n",
    "    relevances = []  # Holds corresponding relevance levels for the ranked docs.\n",
    "    # TODO: complete\n",
    "        \n",
    "    relevances_ideal = []  # Relevance levels of the idealized ranking.\n",
    "    # TODO: complete\n",
    "    \n",
    "    return dcg(relevances, k) / dcg(relevances_ideal, k)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "@pytest.mark.parametrize(\"qid,k,correct_value\", [\n",
    "    (\"q1\", 5, 0.799),\n",
    "    (\"q1\", 10, 0.799),\n",
    "    (\"q2\", 5, 0.549),\n",
    "    (\"q2\", 10, 0.705),\n",
    "    (\"q3\", 5, 0.908),\n",
    "    (\"q3\", 10, 0.949),\n",
    "])\n",
    "def test_queries(qid, k, correct_value):    \n",
    "    assert ndcg(system_rankings[qid], ground_truth[qid], k) == pytest.approx(correct_value, rel=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
